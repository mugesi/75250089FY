---
title: "Data and Measurement"
output:
  html_document:
    theme: 
       bootswatch: minty
    df_print: kable
    code_folding: hide 
---

# Tutorial 3

In this tutorial, we will carry out exercises on criterion and content validity. We will also deal with some data wrangling tasks. Below you will find detailed information on the data sets we will use.

Overview of  datasets

- depression_data.csv: A group of social science researchers working the field of mental health develop a new assessment tool to measure depressive symptoms. To check the concurrent validity of their new tool they run a survey to 300 respondents where they adminsiter their new tool at teh same time with the more established measure, the Beck Depression Inventory (BDI). 

- official_records.csv: This data set contain information on the voter turnout for a sample of citizens from offical records.

- self_reported_voting.csv: This data set contains the results of a survey in which some citizens were asked about their voting behavior in the last elections.

- organizational_data.csv: This dataset contains information on six items to measure organizational knowledge and organizational performance. All items are measured at the organizational level.

Organizational Performance Items:
Financial Performance: This item measures revenue growth.
Customer Satisfaction: This item  measures customer satisfaction scores.
Employee Productivity: This item measures sales per employee.

Organizational Knowledge Items:
Knowledge Sharing: This item measures the extent to which employees share knowledge and information within the organization through channels such as meetings, email, or collaborative platforms.
Learning and Development: This item measures the organization's investment in employee training and development programs, including the number of training hours per employee or the percentage of employees participating in training.
Innovation Culture: This item measures the organization's culture of innovation, including factors such as the encouragement of new ideas, the willingness to take risks, and the support for experimentation.

##  {.tabset .tabset-fade .tabset-pills}
### Exercise 1 - Criterion-related validity

1) Load the "depression_data.csv" into  R environment. 
```{r}
# Read the CSV file into R
depression_data <- read.csv("/Users/msimsek/Desktop/UvA/Teaching/Data_and_Measurement_RMSS/2024/W3/Tutorials/depression_data.csv")
```

2) Print the first few rows of the dataset to have a quick look at the data
```{r}
#Print the first few rows of the dataframe
head(depression_data)
```

3) Visualize the BID results with the new_assessment results (Hint: you can check the relationship between them as well as their distributions).
```{r}
library(ggplot2)
# Create a scatter plot
scatter_plot <- ggplot(data = depression_data, aes(x = New_Assessment_Scores, y = BDI_Scores)) +
  geom_point() +  # Add points
  labs(x = "New Assessment Scores", y = "BDI Scores") +  # Add axis labels
  ggtitle("Scatter Plot of New Assessment Scores vs. BDI Scores")  # Add title

#Show the plot
scatter_plot

# Load the ggplot2 package
library(ggplot2)

# Create a density plot for New Assessment Scores
density_plot <- ggplot(data = depression_data, aes(x = New_Assessment_Scores)) +
  geom_density(aes(color = "New Assessment Scores")) +  # Add density plot for New Assessment Scores
  labs(x = "New Assessment Scores", y = "Density") +  # Add axis labels
  ggtitle("Density Plot of New Assessment Scores and BDI Scores")  # Add title

# Add a density plot for BDI Scores on top of the existing plot
density_plot <- density_plot +
  geom_density(data = depression_data, aes(x = BDI_Scores, color = "BDI Scores"))  # Add density plot for BDI Scores

# Show the plot
print(density_plot)
```

4) What can you say about the concurrent validity of the new assessment scores? (Hint: Do the results of the new assessment tool correlate with the results of the established BDI?)
```{r}
# Compute the correlation between BDI scores and new assessment scores
correlation <- cor(depression_data$BDI_Scores, depression_data$New_Assessment_Scores)

# Print the correlation
cat("Correlation between BDI and New Assessment Scores:", correlation,"\n")

cat("A correlation coefficient of  between the BDI scores and the scores from the new depression assessment tool indicates a very weak positive correlation. 
A weak correlation suggests that the new depression assessment tool is not effectively capturing depressive symptoms in line with the established BDI measure. Therefore, it would be reasonable to conclude that there is low concurrent validity between the new depression assessment tool and the BDI measure.")

```

5) Read the dataset "official_records.csv" into R
```{r}
#Read official_records dataset
official_records <- read.csv("/Users/msimsek/Desktop/UvA/Teaching/Data_and_Measurement_RMSS/2024/W3/Tutorials/official_records.csv")
```

6) Print the first few rows to see the variables and check the number of observations
```{r}
head(official_records)
nrow(official_records)
```

7) Read the "self_reported_voting.csv" dataset
```{r}
# Read self-reported voting dataset
self_reported_voting <- read.csv("/Users/msimsek/Desktop/UvA/Teaching/Data_and_Measurement_RMSS/2024/W3/Tutorials/self_reported_voting.csv")
```

8) Print the first few rows to see the variables, and check the number of observations
```{r}
head(self_reported_voting)
nrow(self_reported_voting)
```

9) Check the common citizenship numbers in two datasets.
```{r}
# Find the indices of the common values in the citizenship variable
common_citizenship_indices <- which(official_records$Citizenship_Number %in% self_reported_voting$Citizenship_Number) 

# Save common citizenship numbers
common_citizenship_numbers <- official_records$Citizenship_Number[
  official_records$Citizenship_Number %in% self_reported_voting$Citizenship_Number]

# check the length
length(common_citizenship_indices)
```


10) Merge the two datasets based on citizenship numbers.(Hint: you can utilize merge() function).What happens to the unmatched cases? Explore the merged dataset
```{r}
# Merging datasets based on citizenship number variable
merged_data <- merge(official_records, self_reported_voting, by = "Citizenship_Number", all.x = TRUE)

# View the structure of the merged dataset
str(merged_data)

# View the first few rows of the merged dataset
head(merged_data)

# Summary statistics of the merged dataset
summary(merged_data)

#Alternatively: you can count the number of NAs in each column of the merged dataset
colSums(is.na(merged_data))
```

11) Analyze Concurrent Validity between self-reported voting behavior and voter turnout from official election records. What could explain the discrepancies between self-reported voting behavior and voter turnout from official election records?  (Hint: Pay attention to the measurement scale of the variables, you can utilize the phi() function from the psych package).
```{r}
cat("The Phi coefficient (Ï†) is used when both variables are dichotomous. It measures the strength and direction of the correlation between two binary variables. A Pearson correlation coefficient estimated for two binary variables provides the phi coefficients.")
#install.packages("psych")
library(psych)
# the phi function requires a 1 x 4 vector or a 2 x 2 matrix as its first argument. Therefore, we use the table function to create a 2 x 2 matrix of the two variables

# Compute the Phi coefficient between official voting and self-reported voting
phi_coefficient <- phi(table(merged_data$Voted_Official, merged_data$Voted_Self_Report))

# Print the Phi coefficient
cat("Phi coefficient between Official Voting and Self-Reported Voting:", phi_coefficient, "\n", "This indicates high concurrent validity, and the discrepancies between self-reported voting and official records might have occurred due to various factors such as recall bias, social desirability bias, and errors in official records.")
```
### Exercise 2 - Construct validity

1) Read the "organizational_data.csv" into  R. 
```{r}
# Read the CSV file into R
organizational_data <- read.csv("/Users/msimsek/Desktop/UvA/Teaching/Data_and_Measurement_RMSS/2024/W3/Tutorials/organizational_data.csv")
```

2) Print the first few rows of the dataset to have a quick look at the data
```{r}
#Print the first few rows of the dataframe
head(organizational_data)
```
3) Analyze construct validity by studying the correlation matrix of all the items

```{r}
# Computing the correlation matrix
correlation_matrix <- cor(organizational_data)
# Printing the correlation matrix
print(correlation_matrix)
```




